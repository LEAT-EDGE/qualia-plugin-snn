[bench]
name        = 'cifar10_QSVGG11_student_10_with_decoupled_potential_v'
seed        = 2
first_run   = 1
last_run    = 3
plugins = ['qualia_plugin_snn']

[learningframework]
kind = 'SpikingJellyMultiStep'
params.devices      = 1
params.precision    = '16-mixed'
#params.use_best_epoch = true

[deploy]
target          = 'Linux'
converter.kind  = 'QualiaCodeGen'
quantize        = ['float32']
optimize        = ['']
compress        = [1]

[dataset]
kind = "CIFAR10"
params.path     = "../dataset/cifar-10-batches-py/"
params.dtype    = "uint8" # Keep uint8 dtype instead of converting to float32 for AutoAugment

[[data_augmentation]]
kind = "HorizontalFlip"
params.before   = true # Run on CPU
params.after    = false

[[data_augmentation]]
kind = "Crop"
params.size     = [32, 32]
params.padding  = [4, 4]
params.before   = true # Run on CPU
params.after    = false

[[data_augmentation]]
kind = "AutoAugment"
params.policy   = "CIFAR10"
params.before   = true # Run on CPU
params.after    = false

# Convert to Float32 and scale by 255 after AutoAugment
[[data_augmentation]]
kind = "IntToFloat32"
params.scale    = true
params.before   = true # Run on CPU
params.after    = false
params.evaluate = true # Also run during inference

[[data_augmentation]]
kind = "Mixup"
params.before   = false
params.after    = true # Run on GPU

[[preprocessing]]
kind = "Class2BinMatrix"

[[postprocessing]]
kind = "EnergyEstimationMetric"
params.mem_width = 16
params.fifo_size = 1000

[[postprocessing]]
kind                = "QuantizationAwareTraining"
export              = true
params.epochs       = 10
params.batch_size   = 64
params.model.params.quant_params.bits           = 8
params.model.params.quant_params.quantype       = "fake"
params.model.params.quant_params.roundtype      = "nearest"
params.model.params.quant_params.range_setting  = "minmax"
params.model.params.quant_params.v.bits           = 16
params.model.params.quant_params.v.quantype           = "fxp"
params.model.params.quant_params.v.LSQ           = false
params.model.params.quant_params.input.quant_enable=false
params.model.params.quant_params.LSQ            = true

[model_template]
kind                = "SCNN"
epochs              = 10
batch_size          = 64
params.dims		    = 2
params.prepool 		= 1
params.timesteps    = 4
params.neuron.kind  = 'LIFNode'
params.neuron.params.v_reset        = false # Soft reset
params.neuron.params.v_threshold    = 1.0
params.neuron.params.tau            = 2.0
params.neuron.params.step_mode      = 'm' # Multi-step mode, make sure to use SpikingJellyMultiStep learningframework
params.neuron.params.backend        = 'torch'

[model_template.optimizer]
kind = "SGD"
params.lr           = 0.075
params.momentum		= 0.9
params.weight_decay	= 0

[model_template.optimizer.scheduler]
kind = "StepLR"
params.gamma        = 0.5
params.step_size    = 30

[[model]]
name = "SVGG11_seed_2_init_e100_act_Logits_L2_lossL2_lambda001"
load = true
train = false
params.filters      = [ 64, 128, 256, 256, 512, 512, 512, 512]
params.kernel_sizes = [  3,   3,   3,   3,   3,   3,   3,   3]
params.paddings     = [  1,   1,   1,   1,   1,   1,   1,   1]
params.strides      = [  2,   2,   1,   2,   1,   2,   1,   2]
params.pool_sizes   = [  0,   0,   0,   0,   0,   0,   0,   0]
params.dropouts     = [  0,   0,   0,   0,   0,   0,   0,   0, 0, 0]
#params.fc_units     = [4096, 4096] # Original VGG
params.fc_units     = [512, 512] # Modified for CIFAR10
params.batch_norm 	= true
disabled = false
