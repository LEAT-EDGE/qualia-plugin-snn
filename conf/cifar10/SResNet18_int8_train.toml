# Spiking VGG16 configuration for CIFAR10 from Andrea Castagnetti, vgg-cifar10-light/vgg16_ifsr_cifar10_lightning.ipynb

[bench]
name = "CIFAR10_SResNet18_int8_train"
seed = 1
first_run = 1
last_run = 1
plugins = ['qualia_plugin_snn']

[learningframework]
kind = 'SpikingJellyMultiStep'
params.devices = 1
params.precision = '16-mixed'

[experimenttracking]
kind = "ClearML"
params.project_name = "CIFAR10"
params.task_name = "CIFAR10_SResNet18_int8_train"
params.offline_mode = true

[deploy]
target = 'Linux'
converter.kind = 'QualiaCodeGen'
quantize = ['float32']
optimize = ['']
compress = [1]

[dataset]
kind = "CIFAR10"
params.path = "data/cifar-10-batches-py/"
params.dtype = "uint8" # Keep uint8 dtype instead of converting to float32 for AutoAugment

[[data_augmentation]]
kind = "HorizontalFlip"
params.before = false # Run on CPU
params.after = true

[[data_augmentation]]
kind = "Crop"
params.size = [32, 32]
params.padding = [4, 4]
params.before = false # Run on CPU
params.after = true

[[data_augmentation]]
kind = "AutoAugment"
params.policy = "CIFAR10"
params.before = false # Run on CPU
params.after = true

# Convert to Float32 and scale by 255 after AutoAugment
[[data_augmentation]]
kind = "IntToFloat32"
params.scale = true
params.before = false # Run on CPU
params.after = true
params.evaluate = true # Also run during inference

[[data_augmentation]]
kind = "Mixup"
params.before = false
params.after = true # Run on GPU

[[preprocessing]]
kind = "Class2BinMatrix"

[[postprocessing]]
kind                = "QuantizationAwareTraining"
export              = true
params.epochs       = 30
params.batch_size   = 128
params.model.params.quant_params.bits           = 16
params.model.params.quant_params.quantype       = "fxp"
params.model.params.quant_params.roundtype      = "floor"
params.model.params.quant_params.range_setting  = "minmax"
params.model.params.quant_params.input.quant_enable=false
params.model.params.quant_params.LSQ            = false

[model_template]
kind                = "SResNet"
epochs              = 0
batch_size          = 64
params.dims		    = 2
params.prepool 		= 1
load = true
train = false
evaluate = false

[model_template.optimizer]
kind = "SGD"
params.lr = 0.0025
params.momentum = 0.9
params.weight_decay	= 0

[model_template.optimizer.scheduler]
kind = "StepLR"
params.gamma = 0.5
params.step_size = 30

# Latent space (before Flatten) is 1×1×512
[[model]]
name = "cifar10_sresnet18"
params.filters      = [64, 64, 128, 256, 512]
params.kernel_sizes = [ 7,  3,   3,   3,   3]
params.paddings     = [ 3,  1,   1,   1,   1]
params.strides      = [ 2,  1,   1,   1,   1]
params.num_blocks   = [     2,   2,   2,   2]
params.postpool     = 'max'
params.batch_norm 	= true
params.timesteps    = 4
params.neuron.kind  = 'IFNode'
params.neuron.params.v_reset = false # Soft reset
params.neuron.params.v_threshold = 1.0
params.neuron.params.detach_reset = true
params.neuron.params.step_mode = 'm' # Multi-step mode, make sure to use SpikingJellyMultiStep learningframework
params.neuron.params.backend = 'torch'
disabled = false
